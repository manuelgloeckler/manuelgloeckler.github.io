{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from rbi.utils.nets import IndependentGaussianNet\n",
    "from rbi.loss.loss_fn import NLLLoss\n",
    "from rbi.defenses.regularized_loss import GaussianNoiseJacobiRegularizer\n",
    "from rbi.utils.fisher_info import fisher_info\n",
    "from rbi.utils.autograd_tools import batch_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = torch.randn((1000, 1))*2\n",
    "p_true = torch.distributions.Normal(means.squeeze(), 2)\n",
    "observations = means + torch.randn(1000, 10)*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(X, \\mu) = p(X|\\mu)p(\\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(torch.nn.Linear(10, 50), torch.nn.ReLU(), torch.nn.Linear(50,50), torch.nn.ReLU(), torch.nn.Linear(50,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2623)\n",
      "tensor(0.2596)\n",
      "tensor(0.2568)\n",
      "tensor(0.2538)\n",
      "tensor(0.2512)\n",
      "tensor(0.2489)\n",
      "tensor(0.2471)\n",
      "tensor(0.2456)\n",
      "tensor(0.2444)\n",
      "tensor(0.2433)\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    pred_mean = net(observations)\n",
    "    p_impl = torch.distributions.Normal(means, 0.5)\n",
    "    loss = -p_impl.log_prob(pred_mean).mean()\n",
    "    if (i % 100) == 0:\n",
    "        print(loss.detach())\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_star = torch.randn((1,1))*2\n",
    "p_true = torch.distributions.Normal(mean_star.squeeze(), 0.5)\n",
    "observations = mean_star + torch.randn(10000, 10)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias:  tensor(0.0154) Variance:  tensor(0.0427) CRLB:  tensor(0.0263)\n"
     ]
    }
   ],
   "source": [
    "observations = observations.requires_grad_(True)\n",
    "bias = net(observations)\n",
    "diff = torch.autograd.grad(bias.sum(), observations)[0].mean(0)\n",
    "var_lower_bound = diff@diff.T*1/4\n",
    "variance = net(observations).var()\n",
    "print(\"Bias: \",torch.mean(bias-mean_star).detach(), \"Variance: \", variance.detach(), \"CRLB: \", var_lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Js = torch.autograd.grad(net(observations).sum(), observations)[0].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5291)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.transpose(Js, -2,-1)@Js).mean()*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-16357.8281, grad_fn=<SumBackward0>) tensor(0.0354, grad_fn=<VarBackward0>) tensor(0.0273)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_expected = batch_jacobian(lambda x: net.predict(x), p_true.sample((1000,10))).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lower_bound = J_expected@torch.transpose(J_expected, -2,-1)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5530]])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8371, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(p_true.sample((1000,10))).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_of_estiamtor = torch.transpose(Js, -2,-1)@torch.linalg.inv(Fs)@Js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_est = (torch.linalg.inv(Fs)@Js)@torch.transpose(Js, -2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_variance_lower_bound = cov_est[:, 0][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9893e-04, 2.5532e-05, 1.6243e-04, 1.5652e-03, 9.9026e-05, 3.3554e-04,\n",
       "        6.8057e-04, 1.9034e-04, 3.9373e-04, 1.4017e-03, 3.0827e-05, 2.3596e-04,\n",
       "        1.1210e-03, 1.2128e-02, 2.3221e-03, 4.4122e-04, 2.9795e-05, 2.0009e-04,\n",
       "        1.4027e-04, 2.9821e-03, 8.3756e-04, 3.2888e-04, 6.4394e-04, 8.2891e-04,\n",
       "        1.5898e-04, 5.5301e-04, 1.5125e-03, 3.9884e-03, 1.0122e-04, 2.9633e-04,\n",
       "        1.4927e-03, 7.2020e-05, 2.6287e-05, 2.2541e-03, 6.5329e-04, 7.4049e-05,\n",
       "        3.3679e-04, 2.9090e-03, 7.5915e-05, 1.5355e-04, 1.1225e-03, 1.5683e-04,\n",
       "        6.0192e-04, 1.0075e-04, 4.3179e-04, 9.8452e-04, 2.1295e-04, 2.3030e-03,\n",
       "        6.7949e-05, 4.6616e-04, 1.9059e-04, 1.0526e-04, 6.9441e-04, 3.8035e-04,\n",
       "        3.0109e-04, 1.2808e-04, 7.1613e-05, 1.8134e-03, 3.1026e-03, 2.8814e-04,\n",
       "        2.0272e-03, 1.0387e-04, 4.5939e-04, 3.6789e-04, 3.3912e-04, 6.3587e-04,\n",
       "        1.1583e-04, 8.2043e-05, 1.3256e-04, 2.8748e-04, 5.7719e-04, 1.5080e-03,\n",
       "        1.0621e-03, 1.4752e-03, 1.2676e-03, 1.7901e-04, 5.5844e-04, 7.4322e-05,\n",
       "        1.5545e-04, 8.0418e-05, 6.3361e-04, 1.6565e-04, 7.0464e-04, 3.1363e-03,\n",
       "        1.0722e-03, 2.0845e-03, 2.1226e-03, 1.1100e-03, 3.5473e-04, 1.4375e-03,\n",
       "        1.8893e-05, 2.4391e-04, 3.7436e-04, 5.6463e-04, 1.9546e-04, 7.4288e-04,\n",
       "        6.3020e-04, 5.5276e-04, 3.3611e-04, 2.5214e-04, 1.1992e-04, 2.1368e-03,\n",
       "        8.0749e-04, 7.9720e-04, 6.3181e-04, 1.0460e-04, 7.4041e-04, 1.8642e-03,\n",
       "        3.7683e-04, 5.7043e-04, 4.9193e-04, 1.0504e-03, 6.8263e-04, 1.8058e-04,\n",
       "        9.3413e-04, 3.6080e-04, 1.9409e-06, 5.0915e-04, 8.0155e-05, 1.1629e-03,\n",
       "        2.7794e-04, 1.0583e-03, 1.0639e-03, 2.2064e-03, 1.7098e-04, 9.0143e-04,\n",
       "        6.2006e-04, 9.4173e-04, 1.9039e-04, 6.8782e-04, 1.0912e-03, 4.6539e-04,\n",
       "        2.2565e-04, 1.1915e-03, 3.8689e-04, 8.7579e-04, 2.6528e-05, 1.2934e-03,\n",
       "        8.1360e-04, 2.6805e-04, 6.2457e-04, 3.9532e-04, 1.4911e-03, 9.3139e-04,\n",
       "        2.8449e-04, 5.6652e-04, 9.0556e-05, 6.8788e-04, 7.9889e-04, 6.9578e-05,\n",
       "        1.8199e-04, 3.5352e-04, 8.5743e-04, 4.9949e-04, 1.3745e-04, 3.0120e-04,\n",
       "        4.5844e-03, 6.7264e-05, 1.5544e-04, 2.6096e-03, 4.4456e-04, 2.1346e-04,\n",
       "        1.2920e-04, 8.3553e-04, 4.3799e-04, 7.3043e-07, 4.2960e-04, 9.6410e-04,\n",
       "        8.0745e-05, 1.2121e-03, 4.4954e-04, 3.2557e-04, 4.9966e-04, 6.5541e-05,\n",
       "        2.4957e-03, 3.3945e-04, 9.6838e-04, 4.4450e-04, 1.0295e-03, 3.9575e-04,\n",
       "        6.5594e-04, 4.8847e-04, 4.1756e-04, 4.9515e-04, 4.2626e-04, 6.6180e-04,\n",
       "        1.7683e-03, 1.7676e-04, 2.6620e-04, 3.9622e-04, 3.4466e-03, 5.9371e-04,\n",
       "        1.5625e-04, 5.4749e-04, 3.0298e-04, 6.4901e-04, 5.4428e-04, 3.9860e-04,\n",
       "        4.4937e-04, 1.7343e-03, 2.2739e-03, 2.8271e-04, 7.5069e-04, 4.2670e-05,\n",
       "        1.8029e-04, 3.7129e-04, 8.2116e-04, 1.1280e-04, 2.7723e-04, 2.5488e-03,\n",
       "        3.7243e-04, 3.2843e-04, 2.3896e-04, 3.0199e-04, 2.9133e-03, 1.9896e-04,\n",
       "        6.6721e-05, 1.2439e-04, 2.7899e-03, 1.0904e-03, 3.5656e-04, 1.4195e-04,\n",
       "        1.0275e-03, 3.7564e-03, 1.5266e-03, 9.8635e-05, 6.6849e-04, 1.0627e-03,\n",
       "        1.6616e-03, 8.4951e-04, 1.4852e-03, 7.6773e-05, 1.2037e-03, 7.6925e-05,\n",
       "        9.5865e-04, 5.7689e-04, 4.2143e-04, 3.1419e-03, 7.9598e-04, 2.4480e-03,\n",
       "        7.9605e-04, 9.3150e-04, 8.2337e-05, 1.7231e-03, 6.6457e-04, 1.2925e-03,\n",
       "        1.1763e-04, 5.4653e-04, 9.9427e-05, 9.7514e-05, 5.0110e-04, 1.5939e-03,\n",
       "        8.9992e-04, 7.5267e-04, 4.6540e-04, 1.6556e-04, 1.8758e-03, 1.6159e-03,\n",
       "        5.3971e-04, 5.1019e-04, 2.3801e-03, 5.8237e-04, 4.6794e-04, 9.8282e-04,\n",
       "        6.5727e-05, 1.3052e-03, 1.0542e-03, 5.7280e-04, 8.9003e-04, 1.0222e-03,\n",
       "        1.8592e-04, 7.9194e-05, 4.5108e-04, 2.0289e-04, 2.1542e-03, 4.2381e-04,\n",
       "        1.1581e-03, 1.0744e-04, 5.9804e-05, 6.6557e-04, 1.2107e-03, 1.8956e-03,\n",
       "        2.9871e-05, 8.7857e-04, 1.0184e-03, 1.6114e-03, 2.4515e-05, 4.8353e-05,\n",
       "        3.3656e-03, 1.2726e-03, 7.3308e-05, 6.4444e-04, 4.3586e-04, 1.5301e-03,\n",
       "        2.7324e-04, 6.7636e-04, 1.0404e-03, 1.4031e-03, 1.0844e-03, 3.3665e-05,\n",
       "        8.7621e-04, 1.6448e-03, 5.4788e-04, 1.0979e-04, 1.6085e-03, 1.5649e-03,\n",
       "        2.3263e-04, 4.8452e-04, 8.8503e-04, 3.5391e-04, 8.7647e-05, 2.2929e-04,\n",
       "        1.3757e-03, 1.6691e-04, 9.8502e-05, 7.0354e-04, 4.2915e-04, 4.3908e-04,\n",
       "        4.6171e-04, 2.9545e-04, 2.7496e-03, 2.7900e-04, 2.5824e-03, 1.2839e-03,\n",
       "        1.7297e-03, 8.7870e-05, 1.5735e-03, 9.2912e-04, 8.6021e-04, 8.7915e-05,\n",
       "        4.7106e-04, 1.6988e-03, 2.2573e-04, 9.4012e-05, 2.2894e-05, 1.4916e-03,\n",
       "        5.7872e-03, 3.0283e-03, 1.0074e-04, 4.2106e-04, 1.7666e-03, 4.0156e-04,\n",
       "        1.6800e-04, 1.8410e-04, 9.9662e-04, 8.2109e-04, 5.1167e-04, 1.7620e-03,\n",
       "        1.4488e-04, 4.1831e-03, 1.7603e-03, 1.4284e-04, 1.1066e-04, 6.6595e-05,\n",
       "        2.3600e-04, 3.0445e-04, 5.2337e-04, 7.4981e-04, 9.7494e-05, 2.3387e-04,\n",
       "        4.4773e-04, 1.3247e-04, 6.7898e-06, 1.3869e-04, 2.5131e-03, 4.5280e-04,\n",
       "        1.6228e-04, 2.3496e-03, 2.3611e-04, 2.7090e-03, 1.9599e-03, 1.7626e-03,\n",
       "        3.9550e-04, 1.2836e-03, 2.9010e-04, 1.3113e-04, 1.2233e-03, 3.3038e-04,\n",
       "        9.5878e-05, 1.6722e-03, 6.7372e-04, 9.1252e-04, 2.0315e-03, 3.0437e-04,\n",
       "        2.0048e-03, 1.3914e-03, 2.9654e-05, 2.1554e-03, 8.7130e-04, 5.9959e-04,\n",
       "        1.1049e-03, 1.0712e-04, 6.0026e-04, 1.6402e-04, 2.9553e-05, 5.9658e-04,\n",
       "        1.3715e-03, 1.3248e-03, 2.3769e-04, 6.6183e-04, 7.3397e-05, 1.0207e-03,\n",
       "        7.6927e-05, 2.9508e-04, 2.7097e-04, 1.6905e-04, 7.6397e-04, 1.3258e-03,\n",
       "        1.9743e-03, 3.6435e-04, 3.7342e-03, 7.1295e-04, 5.2607e-04, 5.8069e-04,\n",
       "        4.2405e-03, 1.6911e-04, 1.3415e-03, 4.9072e-04, 8.1681e-04, 4.1095e-04,\n",
       "        2.5435e-05, 3.6495e-04, 3.8225e-03, 1.3159e-04, 1.1933e-03, 5.1257e-03,\n",
       "        9.8749e-05, 2.2481e-03, 2.3374e-04, 2.0667e-04, 6.8854e-04, 5.4904e-04,\n",
       "        1.7484e-03, 2.0119e-03, 2.2134e-04, 1.3438e-03, 3.4990e-04, 5.8010e-05,\n",
       "        1.6276e-04, 5.8357e-05, 2.8218e-03, 5.8216e-04, 2.8612e-04, 6.3643e-04,\n",
       "        1.9197e-03, 8.2965e-06, 3.3368e-05, 1.2470e-03, 8.1149e-03, 1.3533e-05,\n",
       "        1.5353e-05, 1.0871e-03, 4.1681e-04, 1.6950e-04, 1.1199e-05, 7.1522e-03,\n",
       "        2.1393e-03, 1.0177e-04, 1.3838e-05, 5.2220e-04, 5.4361e-04, 1.7280e-03,\n",
       "        2.8246e-04, 3.2785e-04, 3.7076e-03, 1.4291e-03, 7.7267e-04, 5.8868e-05,\n",
       "        7.8391e-04, 2.8460e-03, 3.0744e-04, 1.0848e-04, 2.6624e-05, 8.2399e-04,\n",
       "        3.2798e-04, 6.2967e-04, 1.0894e-03, 9.9008e-04, 8.4368e-04, 5.4101e-05,\n",
       "        1.1479e-03, 6.7131e-05, 6.1153e-04, 4.3800e-05, 5.3013e-04, 6.2766e-04,\n",
       "        6.6244e-04, 6.7543e-04, 4.0769e-04, 2.2352e-04, 1.8385e-03, 2.3642e-03,\n",
       "        8.1818e-05, 7.2522e-04, 1.5762e-04, 6.7651e-05, 4.4239e-04, 1.1389e-04,\n",
       "        3.6177e-04, 9.4068e-05, 7.4322e-04, 3.7420e-03, 3.3149e-03, 5.0838e-04,\n",
       "        2.9829e-03, 8.8280e-04, 4.8801e-05, 8.4379e-05, 3.9837e-04, 2.0356e-04,\n",
       "        1.2481e-03, 1.6198e-04, 4.6894e-04, 3.5572e-05, 2.9998e-03, 1.1150e-03,\n",
       "        1.4942e-04, 8.6667e-04, 8.8924e-05, 2.1558e-04, 2.1369e-03, 3.7813e-04,\n",
       "        1.6895e-03, 1.4449e-04, 9.0932e-04, 3.6534e-03, 1.9725e-03, 1.2248e-04,\n",
       "        2.0906e-03, 1.8249e-03, 2.1130e-03, 5.1558e-04, 6.9101e-03, 2.7516e-04,\n",
       "        3.8790e-04, 4.0481e-04, 9.4806e-03, 6.2216e-04, 3.7294e-05, 2.8120e-03,\n",
       "        4.2718e-04, 1.8297e-03, 1.7328e-03, 5.2105e-04, 6.5281e-04, 2.9173e-04,\n",
       "        1.1378e-04, 1.4368e-03, 3.7513e-04, 2.4064e-04, 2.5919e-03, 3.4006e-04,\n",
       "        2.1095e-03, 9.1615e-04, 7.9292e-04, 8.0544e-04, 1.2383e-03, 3.0500e-04,\n",
       "        8.9494e-05, 1.2455e-03, 7.0426e-04, 2.1198e-04, 1.4925e-05, 1.0094e-03,\n",
       "        3.7411e-05, 3.8802e-05, 9.1917e-05, 2.9226e-04, 2.1139e-03, 2.1063e-03,\n",
       "        2.9753e-04, 9.3134e-05, 1.5343e-04, 5.4948e-04, 5.3963e-04, 5.0791e-04,\n",
       "        2.0702e-04, 6.2048e-04, 1.2118e-03, 3.7501e-03, 5.2277e-04, 1.0861e-03,\n",
       "        5.9216e-03, 3.1557e-03, 6.9050e-04, 1.3884e-03, 3.0506e-04, 1.9807e-03,\n",
       "        7.2959e-05, 1.2298e-03, 1.3579e-03, 3.0622e-04, 4.1890e-04, 7.6272e-05,\n",
       "        9.5686e-05, 2.9884e-04, 8.1944e-05, 2.0010e-04, 4.3496e-04, 2.1093e-03,\n",
       "        1.5992e-03, 2.9536e-03, 7.5569e-05, 6.2259e-04, 6.8095e-04, 2.9236e-04,\n",
       "        2.1315e-04, 5.5163e-06, 3.6481e-04, 2.4139e-05, 5.1265e-05, 8.4659e-04,\n",
       "        1.0915e-03, 1.2545e-03, 1.3616e-03, 1.8920e-04, 2.6727e-04, 2.4771e-03,\n",
       "        9.2024e-05, 3.1832e-03, 9.5651e-05, 3.7591e-04, 2.3030e-03, 6.8729e-04,\n",
       "        1.7723e-04, 1.1766e-05, 3.6719e-04, 2.6440e-05, 5.8848e-05, 8.3345e-04,\n",
       "        6.5536e-04, 2.3664e-05, 1.1359e-04, 3.7851e-04, 1.0450e-04, 1.4947e-03,\n",
       "        1.5630e-04, 1.6244e-03, 2.3814e-04, 7.0655e-04, 8.5270e-04, 4.6849e-04,\n",
       "        2.4325e-03, 1.1923e-03, 8.7980e-04, 5.7349e-04, 1.8309e-03, 4.5309e-05,\n",
       "        2.0970e-03, 9.2016e-04, 1.9040e-03, 3.5220e-04, 2.4446e-05, 3.7072e-05,\n",
       "        6.9861e-04, 1.8948e-04, 4.1900e-04, 6.3075e-04, 3.3456e-04, 8.5357e-04,\n",
       "        7.1098e-05, 3.5969e-04, 1.1114e-04, 1.1212e-04, 1.9025e-04, 1.4202e-03,\n",
       "        3.9170e-04, 8.0989e-05, 6.9965e-04, 3.2060e-05, 3.7468e-04, 2.0446e-03,\n",
       "        3.6456e-05, 5.5076e-04, 3.5260e-04, 6.4774e-04, 2.4739e-03, 3.1853e-04,\n",
       "        1.2120e-03, 1.5759e-03, 1.9048e-03, 5.1814e-04, 3.2618e-04, 7.3069e-04,\n",
       "        8.3218e-04, 3.7099e-04, 9.6857e-04, 3.8299e-04, 2.3433e-04, 2.0035e-04,\n",
       "        1.8020e-04, 6.9247e-04, 3.3208e-04, 7.6407e-04, 2.8252e-04, 9.4943e-05,\n",
       "        7.0451e-04, 2.4020e-04, 4.3490e-05, 2.5530e-05, 7.6509e-04, 8.8515e-04,\n",
       "        7.8695e-04, 1.5808e-03, 6.6914e-04, 3.5898e-04, 7.8679e-05, 5.4162e-04,\n",
       "        1.1473e-03, 1.3371e-05, 5.6678e-04, 6.9594e-04, 1.3981e-04, 1.6954e-05,\n",
       "        5.1761e-04, 3.7997e-03, 1.0442e-05, 2.3189e-04, 3.6595e-03, 1.0585e-04,\n",
       "        1.8705e-04, 1.3902e-03, 1.8572e-03, 6.5075e-04, 6.1017e-04, 1.3462e-04,\n",
       "        1.3219e-03, 6.0672e-04, 4.1933e-04, 3.1494e-03, 4.6615e-04, 3.0274e-04,\n",
       "        3.0591e-04, 6.5762e-04, 3.2073e-04, 1.6514e-04, 6.3632e-04, 1.5958e-03,\n",
       "        2.2741e-04, 1.3559e-03, 1.0399e-03, 1.3415e-03, 5.7615e-04, 2.4656e-04,\n",
       "        1.2044e-03, 4.5684e-04, 1.3658e-04, 6.4695e-04, 1.1986e-04, 3.7780e-04,\n",
       "        8.5081e-05, 9.2003e-04, 8.0123e-05, 1.0245e-03, 7.9662e-04, 5.2156e-04,\n",
       "        3.3177e-05, 2.2673e-03, 1.0762e-03, 4.7614e-04, 7.2896e-04, 4.4625e-05,\n",
       "        1.1523e-02, 9.6952e-05, 4.0877e-04, 1.0046e-04, 7.4085e-05, 2.8208e-04,\n",
       "        9.1597e-05, 2.4195e-03, 4.5335e-04, 4.6558e-04, 4.8267e-04, 2.2533e-04,\n",
       "        1.3193e-04, 2.3737e-05, 6.1564e-04, 1.3797e-03, 1.6091e-04, 3.1283e-03,\n",
       "        1.1575e-03, 4.5483e-04, 2.0561e-04, 3.7213e-04, 2.9346e-03, 1.6112e-03,\n",
       "        1.5914e-03, 2.3116e-04, 1.5416e-03, 6.3492e-04, 2.7732e-05, 5.9641e-05,\n",
       "        2.3532e-04, 5.2649e-05, 9.0074e-05, 1.3782e-06, 1.6812e-03, 2.5894e-04,\n",
       "        5.4455e-04, 8.0828e-05, 1.1942e-03, 1.3491e-04, 5.6172e-04, 6.2191e-03,\n",
       "        2.2806e-04, 1.3731e-03, 2.1403e-04, 1.0584e-03, 5.9869e-04, 4.0420e-04,\n",
       "        1.6933e-04, 6.7744e-04, 1.0751e-04, 2.5818e-04, 5.1634e-04, 2.3006e-03,\n",
       "        1.6383e-04, 4.8139e-04, 1.2657e-03, 5.2975e-04, 2.4831e-04, 2.1179e-04,\n",
       "        4.4278e-04, 5.7727e-04, 3.4210e-04, 2.4485e-05, 6.7757e-04, 7.6187e-04,\n",
       "        3.1694e-04, 1.7966e-04, 3.0207e-03, 9.4905e-05, 1.5842e-03, 3.3102e-05,\n",
       "        2.5174e-03, 6.0564e-04, 2.6203e-04, 1.1243e-03, 2.7840e-03, 2.9133e-04,\n",
       "        1.8989e-03, 7.0463e-05, 1.3580e-04, 2.7237e-04, 2.8785e-04, 3.4207e-03,\n",
       "        2.4798e-04, 1.2773e-03, 2.3063e-04, 9.6891e-04, 3.0512e-03, 2.0417e-04,\n",
       "        2.2543e-04, 1.5272e-03, 1.9449e-03, 3.1671e-04, 9.2631e-04, 8.4236e-04,\n",
       "        1.3595e-04, 2.7878e-04, 2.0074e-04, 7.3803e-04, 6.8457e-04, 4.5058e-04,\n",
       "        3.1554e-05, 2.3249e-04, 6.7217e-04, 1.6897e-04, 1.4699e-04, 1.8490e-03,\n",
       "        1.2676e-04, 1.8959e-03, 5.9582e-04, 3.4473e-04, 4.2865e-04, 7.6315e-04,\n",
       "        2.1966e-05, 9.4492e-05, 5.6149e-04, 5.2810e-03, 5.2337e-04, 6.5024e-04,\n",
       "        3.8765e-04, 3.0080e-04, 6.8545e-04, 1.8745e-03, 1.5623e-03, 1.3211e-05,\n",
       "        4.2001e-04, 2.1679e-04, 2.0799e-03, 7.2597e-04, 2.2832e-04, 4.8602e-04,\n",
       "        3.3816e-03, 5.5865e-04, 8.6142e-04, 1.2020e-03, 2.6918e-03, 2.5010e-04,\n",
       "        1.0624e-03, 6.0077e-04, 4.8114e-04, 4.0989e-04, 6.2292e-05, 1.3675e-03,\n",
       "        3.4097e-04, 5.4949e-04, 7.5033e-04, 2.4472e-04, 8.2556e-04, 2.1054e-04,\n",
       "        5.2393e-04, 6.8884e-04, 3.6415e-05, 1.0640e-03, 3.2666e-04, 3.2704e-04,\n",
       "        2.2706e-04, 1.4610e-04, 8.7601e-05, 6.8252e-05, 8.5396e-05, 4.2888e-04,\n",
       "        6.8541e-05, 2.0299e-03, 3.3085e-04, 3.8711e-03, 1.3987e-04, 1.8964e-03,\n",
       "        1.7775e-03, 6.6049e-04, 3.1431e-04, 3.6391e-04, 8.4556e-04, 1.4258e-03,\n",
       "        1.9673e-04, 6.4137e-04, 1.1841e-04, 1.2946e-04, 8.5529e-04, 2.4532e-04,\n",
       "        3.0968e-03, 2.0542e-05, 1.5994e-03, 1.1290e-03, 9.7954e-04, 5.6268e-04,\n",
       "        1.9882e-03, 5.4538e-04, 2.5113e-04, 7.3027e-04, 3.2681e-05, 1.4273e-03,\n",
       "        1.2337e-04, 1.1834e-04, 1.3172e-03, 3.7612e-04, 3.7947e-04, 2.7267e-04,\n",
       "        1.7529e-03, 1.6752e-04, 1.3678e-04, 1.2577e-05, 5.1525e-03, 3.4453e-04,\n",
       "        1.8196e-03, 1.8956e-03, 4.1861e-04, 6.9966e-04, 2.3670e-04, 1.1096e-03,\n",
       "        1.7434e-03, 3.3862e-05, 2.1937e-04, 1.2017e-03, 1.8591e-04, 2.3748e-04,\n",
       "        4.1521e-04, 3.6600e-04, 1.4018e-04, 2.9459e-04, 1.7571e-04, 7.3459e-04,\n",
       "        6.4801e-04, 7.3352e-04, 1.5147e-04, 1.1296e-04, 6.8722e-04, 1.2838e-03,\n",
       "        2.5247e-04, 3.1133e-04, 1.9513e-03, 4.6164e-04, 8.8172e-04, 2.1040e-03,\n",
       "        2.4008e-04, 3.4963e-03, 5.7224e-05, 3.0988e-03],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_variance_lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0457, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(observations).mean.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0128, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((net(observations).base_dist.loc - observations.mean(-1).unsqueeze(-1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = IndependentGaussianNet(10, 1)\n",
    "loss_fn = NLLLoss(net2)\n",
    "defense = GaussianNoiseJacobiRegularizer(net, loss_fn, 0.2)\n",
    "defense.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4554)\n",
      "tensor(1.3074)\n",
      "tensor(1.2321)\n",
      "tensor(1.2172)\n",
      "tensor(1.2070)\n",
      "tensor(1.1984)\n",
      "tensor(1.1916)\n",
      "tensor(1.1857)\n",
      "tensor(1.1810)\n",
      "tensor(1.1766)\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(net2.parameters(), lr=1e-3)\n",
    "for i in range(1000):\n",
    "    optim.zero_grad()\n",
    "    loss = loss_fn(observations, means)\n",
    "    if (i % 100) == 0:\n",
    "        print(loss.detach())\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = defense._compute_fisher(observations[0].unsqueeze(0), net2(observations[0].unsqueeze(0))).detach() + torch.eye(10)*0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_inv = torch.linalg.inv(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.2797e-02, 9.0554e-01, 9.9991e+02, 9.9994e+02, 9.9998e+02, 9.9998e+02,\n",
       "         1.0000e+03, 1.0000e+03, 1.0001e+03, 1.0003e+03]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.eigvalsh(F_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.distributions.MultivariateNormal(observations[0], F_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5746],\n",
       "        [13.2651],\n",
       "        [ 1.9085],\n",
       "        [ 3.7335],\n",
       "        [ 9.9578],\n",
       "        [ 0.5435],\n",
       "        [14.9603],\n",
       "        [-1.0185],\n",
       "        [-0.3865],\n",
       "        [-0.0564]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sample((10,)).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7794],\n",
       "        [-1.7057],\n",
       "        [-1.9992],\n",
       "        [-1.6891],\n",
       "        [-1.8192],\n",
       "        [-1.7018],\n",
       "        [-1.7421],\n",
       "        [-1.9387],\n",
       "        [-1.6105],\n",
       "        [-1.7220],\n",
       "        [-1.9977],\n",
       "        [-1.6809],\n",
       "        [-1.7099],\n",
       "        [-1.6032],\n",
       "        [-1.8177],\n",
       "        [-1.8952],\n",
       "        [-1.7103],\n",
       "        [-1.7752],\n",
       "        [-1.8190],\n",
       "        [-1.7495],\n",
       "        [-1.8155],\n",
       "        [-1.7258],\n",
       "        [-1.6928],\n",
       "        [-1.7232],\n",
       "        [-1.7981],\n",
       "        [-1.7462],\n",
       "        [-1.8113],\n",
       "        [-1.6597],\n",
       "        [-1.6997],\n",
       "        [-1.8801],\n",
       "        [-1.7440],\n",
       "        [-1.7890],\n",
       "        [-1.5049],\n",
       "        [-1.8259],\n",
       "        [-1.7430],\n",
       "        [-1.7593],\n",
       "        [-1.6150],\n",
       "        [-1.6529],\n",
       "        [-1.7349],\n",
       "        [-1.8002],\n",
       "        [-1.5405],\n",
       "        [-1.6300],\n",
       "        [-1.9168],\n",
       "        [-1.8031],\n",
       "        [-1.5336],\n",
       "        [-1.7707],\n",
       "        [-1.7355],\n",
       "        [-1.8818],\n",
       "        [-1.6828],\n",
       "        [-1.8101],\n",
       "        [-1.9117],\n",
       "        [-1.6224],\n",
       "        [-1.7857],\n",
       "        [-1.7167],\n",
       "        [-1.7403],\n",
       "        [-1.6923],\n",
       "        [-1.7097],\n",
       "        [-1.5695],\n",
       "        [-1.7946],\n",
       "        [-1.8335],\n",
       "        [-1.7449],\n",
       "        [-1.8445],\n",
       "        [-1.8537],\n",
       "        [-1.7812],\n",
       "        [-1.8252],\n",
       "        [-1.6802],\n",
       "        [-1.7109],\n",
       "        [-1.6611],\n",
       "        [-1.6424],\n",
       "        [-1.7599],\n",
       "        [-1.9609],\n",
       "        [-1.6981],\n",
       "        [-1.7313],\n",
       "        [-1.6006],\n",
       "        [-1.6678],\n",
       "        [-1.8414],\n",
       "        [-1.8683],\n",
       "        [-1.7680],\n",
       "        [-1.8630],\n",
       "        [-1.8192],\n",
       "        [-1.6732],\n",
       "        [-1.8397],\n",
       "        [-1.8056],\n",
       "        [-1.5878],\n",
       "        [-1.8275],\n",
       "        [-1.8350],\n",
       "        [-1.5778],\n",
       "        [-1.7513],\n",
       "        [-1.9022],\n",
       "        [-1.9001],\n",
       "        [-1.6205],\n",
       "        [-1.7743],\n",
       "        [-1.7043],\n",
       "        [-1.7867],\n",
       "        [-1.7240],\n",
       "        [-1.9447],\n",
       "        [-1.5611],\n",
       "        [-1.7631],\n",
       "        [-1.9912],\n",
       "        [-1.5916]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(observations[1].unsqueeze(0) + 0.1*torch.randn(100,1)).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7549],\n",
       "        [-1.5716],\n",
       "        [-1.5987],\n",
       "        [-1.2383],\n",
       "        [-0.8455],\n",
       "        [-1.7040],\n",
       "        [-1.1621],\n",
       "        [-1.6986],\n",
       "        [-1.4179],\n",
       "        [-1.4974],\n",
       "        [-1.8474],\n",
       "        [-2.1858],\n",
       "        [-0.8035],\n",
       "        [-1.5460],\n",
       "        [-1.7395],\n",
       "        [-1.9392],\n",
       "        [-1.5967],\n",
       "        [-1.3721],\n",
       "        [-1.3601],\n",
       "        [-1.6426],\n",
       "        [-1.1538],\n",
       "        [-1.6184],\n",
       "        [-1.9061],\n",
       "        [-1.5908],\n",
       "        [-1.4998],\n",
       "        [-1.7900],\n",
       "        [-2.1121],\n",
       "        [-1.7119],\n",
       "        [-1.4837],\n",
       "        [-1.3873],\n",
       "        [-1.7578],\n",
       "        [-1.6242],\n",
       "        [-1.6461],\n",
       "        [-1.6029],\n",
       "        [-1.9264],\n",
       "        [-1.6585],\n",
       "        [-1.4569],\n",
       "        [-1.4690],\n",
       "        [-1.5868],\n",
       "        [-1.4912],\n",
       "        [-2.0929],\n",
       "        [-1.4760],\n",
       "        [-1.8047],\n",
       "        [-1.5554],\n",
       "        [-1.7501],\n",
       "        [-1.5341],\n",
       "        [-1.7682],\n",
       "        [-2.0359],\n",
       "        [-1.6556],\n",
       "        [-2.0150],\n",
       "        [-1.8584],\n",
       "        [-1.8833],\n",
       "        [-1.8965],\n",
       "        [-1.7424],\n",
       "        [-1.4725],\n",
       "        [-1.7008],\n",
       "        [-1.6702],\n",
       "        [-1.9409],\n",
       "        [-1.5567],\n",
       "        [-1.7517],\n",
       "        [-1.6849],\n",
       "        [-1.2114],\n",
       "        [-1.1585],\n",
       "        [-1.7685],\n",
       "        [-1.8029],\n",
       "        [-1.9554],\n",
       "        [-1.9271],\n",
       "        [-1.6515],\n",
       "        [-1.3610],\n",
       "        [-1.6465],\n",
       "        [-1.2729],\n",
       "        [-1.7592],\n",
       "        [-0.8100],\n",
       "        [-1.4987],\n",
       "        [-2.0011],\n",
       "        [-1.9300],\n",
       "        [-1.5903],\n",
       "        [-1.3472],\n",
       "        [-1.7727],\n",
       "        [-1.6142],\n",
       "        [-1.5380],\n",
       "        [-1.7332],\n",
       "        [-1.2242],\n",
       "        [-1.4979],\n",
       "        [-1.5444],\n",
       "        [-1.3807],\n",
       "        [-0.7107],\n",
       "        [-1.6021],\n",
       "        [-1.6324],\n",
       "        [-1.2717],\n",
       "        [-1.2884],\n",
       "        [-1.5477],\n",
       "        [-1.9054],\n",
       "        [-2.1601],\n",
       "        [-1.3678],\n",
       "        [-1.4003],\n",
       "        [-2.0884],\n",
       "        [-1.1022],\n",
       "        [-1.3601],\n",
       "        [-1.3036]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(p.sample((100,))).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0284, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((net2(observations).base_dist.loc - observations.median(-1).values.unsqueeze(-1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0127, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((net2(observations).base_dist.loc - observations.mean(-1).unsqueeze(-1))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.median(\n",
       "values=tensor([-4.4001e+00, -3.2258e+00, -2.1351e+00, -2.3956e+00, -2.6880e+00,\n",
       "         1.4055e+00,  2.0184e+00, -3.2590e+00, -2.3928e-01,  3.8684e+00,\n",
       "         1.1583e+00,  4.7608e-01, -2.6399e-01,  1.6707e+00, -2.7737e+00,\n",
       "        -3.0504e+00,  6.3489e-01,  2.7195e+00,  6.5361e+00,  1.4531e+00,\n",
       "        -4.3600e-01, -2.3690e+00, -1.7195e+00, -1.2741e+00, -2.0122e+00,\n",
       "        -1.5859e+00,  3.9849e-01,  2.2415e+00,  1.2183e+00, -3.0741e+00,\n",
       "         1.4684e+00,  5.6325e-01,  1.8900e-01,  1.4461e+00,  3.4787e-01,\n",
       "         2.2897e+00,  1.8708e+00, -5.5868e-01, -2.2019e+00,  1.2163e+00,\n",
       "        -2.3448e+00,  1.4460e+00, -1.0969e+00,  1.2090e+00, -9.8263e-01,\n",
       "         1.8689e-01, -2.3600e+00,  2.8903e+00,  1.2310e+00,  1.1050e+00,\n",
       "         1.9051e+00, -4.9865e-01,  8.2530e-01, -2.6378e-01,  5.9296e-01,\n",
       "         1.5881e+00,  3.2970e+00, -1.9274e+00, -1.2162e+00, -5.6231e-01,\n",
       "         1.7530e+00, -2.2519e-01, -1.2262e+00,  7.7451e-01, -3.5798e-01,\n",
       "         1.1902e+00,  2.3770e+00, -4.3995e-01, -1.1788e+00,  1.5611e+00,\n",
       "         9.8469e-01, -3.7550e+00,  1.7052e+00,  1.1193e+00,  4.2739e+00,\n",
       "         1.4226e+00,  2.9811e+00,  2.5950e-01,  2.8009e+00, -1.5124e+00,\n",
       "        -1.8421e-01, -1.8306e+00, -1.6051e+00, -4.2737e-01, -2.4831e-02,\n",
       "         3.5466e-01,  4.5499e-02, -3.9040e+00, -1.6104e+00, -3.7219e+00,\n",
       "        -4.7126e+00, -2.1126e+00,  2.1543e+00, -4.5904e+00,  1.1223e+00,\n",
       "         7.0710e-01,  9.2294e-02,  3.3744e-01, -9.5196e-01, -1.7067e+00,\n",
       "         1.4392e+00,  3.4599e-01,  1.1941e+00, -4.5395e-01, -2.5905e+00,\n",
       "         1.2051e+00, -1.0455e+00, -3.1484e-01,  2.5778e+00,  6.0021e+00,\n",
       "         5.5230e-02,  5.8162e-01, -1.1498e+00, -1.6316e+00,  5.1849e+00,\n",
       "         1.2421e+00, -3.3663e+00,  3.0061e-01,  5.5087e-01, -1.1813e+00,\n",
       "         1.0472e+00,  2.0330e+00,  1.2012e+00,  2.3379e+00, -1.2241e+00,\n",
       "         2.3085e+00, -8.3890e-02,  2.5861e-01,  2.5372e+00, -8.8708e-02,\n",
       "         2.9724e+00,  4.0001e-01,  2.1380e+00,  1.0133e+00, -2.4135e+00,\n",
       "         7.4007e+00, -5.2673e-01,  1.7875e+00, -1.7299e-01, -5.7103e-01,\n",
       "         1.0387e+00,  4.0641e-01, -4.5747e-01, -2.1040e-01,  3.2322e+00,\n",
       "        -1.3765e+00,  6.0619e-01,  1.1875e+00, -2.3367e+00, -2.3647e+00,\n",
       "        -1.1497e+00,  2.5843e+00,  2.1467e+00, -2.2681e+00, -1.0949e+00,\n",
       "         5.9139e-01,  1.3630e+00, -1.1293e+00, -3.3018e+00, -1.2415e+00,\n",
       "         1.1023e+00,  7.3632e-01,  1.1204e+00, -3.5507e+00,  3.1086e+00,\n",
       "         9.0556e-02, -9.7737e-01, -3.3007e-01,  6.9901e-01, -8.7107e-01,\n",
       "        -7.3866e-01,  2.1573e-01,  2.3166e+00, -3.3006e+00, -1.0326e+00,\n",
       "        -1.7355e+00,  1.3836e-01, -3.5476e+00,  8.4033e-01, -1.6568e+00,\n",
       "         2.8945e+00, -1.4409e+00, -4.5907e+00,  2.0161e+00,  1.2422e+00,\n",
       "         2.2832e+00,  4.9246e-01,  1.5331e-02,  2.3587e+00, -3.9860e-01,\n",
       "         3.2285e+00, -8.7340e-01,  3.7014e-01, -2.1458e-01, -1.5802e+00,\n",
       "        -1.6716e+00, -8.9017e-01,  1.6757e+00,  5.8127e-01, -1.2142e-01,\n",
       "        -1.1773e+00, -3.3376e-01, -8.7823e-01, -1.6683e+00, -1.0975e-01,\n",
       "        -3.8471e+00, -1.8459e+00,  3.2752e+00,  4.9307e+00,  6.0955e-01,\n",
       "         5.7544e-01, -7.2788e-01, -3.0598e+00,  2.6866e-01,  9.5094e-01,\n",
       "        -3.8906e+00,  3.4747e+00,  6.1832e-01, -3.0418e-01, -7.7322e-01,\n",
       "         1.8550e+00, -3.8203e+00, -2.2355e+00,  2.1079e+00, -2.4462e-01,\n",
       "         8.7711e-01, -9.0595e-01,  1.3045e+00, -1.8571e+00, -1.6240e-01,\n",
       "         2.0378e+00,  1.0260e+00,  2.2503e+00, -1.7848e+00, -3.4640e+00,\n",
       "         2.3315e+00,  6.4740e-01,  2.2780e+00,  6.5007e-01, -2.2208e-01,\n",
       "         2.1559e+00, -8.6930e-01,  1.5333e+00,  1.2899e+00, -4.1516e-01,\n",
       "         8.7323e-01,  6.5539e-01,  9.0920e-01, -7.6709e-01,  1.7855e+00,\n",
       "         2.9748e-01,  2.5489e+00,  8.1864e-01,  6.3220e-01, -1.4879e-01,\n",
       "        -2.5863e+00, -2.7043e+00, -1.3196e-01,  2.3173e+00,  1.2505e+00,\n",
       "        -6.6713e-01, -1.1756e+00, -2.6481e+00,  7.9394e-02,  3.8089e+00,\n",
       "        -4.2087e-01, -2.1827e+00, -2.0613e+00, -4.0879e-01, -2.9647e-01,\n",
       "         1.9783e+00, -1.8935e+00, -1.9551e+00,  2.8547e+00,  1.1431e+00,\n",
       "         4.4791e+00, -4.6537e+00, -1.7343e+00, -1.5550e+00,  1.5177e+00,\n",
       "        -2.2251e+00, -2.7483e+00,  4.3128e+00,  2.5718e-01, -4.0981e-01,\n",
       "        -5.3936e-01, -4.1021e-01, -6.6232e-01, -6.4055e-01, -2.9440e+00,\n",
       "        -4.1985e+00,  3.3294e+00, -2.4463e+00, -5.9248e-01, -3.2481e+00,\n",
       "         3.2846e+00,  2.9676e+00, -1.5041e+00, -3.0758e+00,  3.2866e+00,\n",
       "         3.9741e-02,  3.7234e+00, -1.2901e+00, -1.3954e+00, -9.7329e-01,\n",
       "        -2.7314e+00, -2.3215e+00, -1.2978e+00,  8.1227e-01,  7.2679e-01,\n",
       "         1.2851e+00,  1.2011e+00, -9.3121e-01,  1.2235e+00, -5.4801e-01,\n",
       "         8.7806e-02, -1.8842e+00,  6.8669e-01,  1.2434e+00, -1.5811e+00,\n",
       "        -3.1782e+00,  1.6791e-02,  1.1449e+00, -7.5289e-01, -1.9531e+00,\n",
       "        -4.4216e-01, -4.6696e+00, -7.7856e-01, -3.9553e-01, -1.6498e+00,\n",
       "         2.6062e+00, -2.8218e+00,  1.3842e+00,  5.6705e-01, -4.8142e+00,\n",
       "         2.7859e+00,  2.6923e+00, -9.5595e-01, -1.8054e+00, -1.9582e+00,\n",
       "         1.7210e+00,  2.6455e+00, -5.7573e-01,  2.0546e+00,  9.9889e-01,\n",
       "        -2.0194e+00, -1.4809e+00, -1.0548e+00, -4.6243e-01,  5.8969e-01,\n",
       "         1.1159e+00,  3.3379e+00,  1.9079e+00,  1.5423e+00, -8.9992e-01,\n",
       "        -2.6192e+00, -1.4587e+00, -2.0005e+00, -2.0050e+00, -2.9460e+00,\n",
       "        -1.1280e-01,  1.2035e-01, -2.5431e+00, -9.7569e-01, -2.3628e+00,\n",
       "        -2.6505e+00,  1.3292e-01, -2.1221e+00,  7.5862e-01,  4.2222e-01,\n",
       "        -5.5886e+00, -1.7874e+00, -9.4163e-01, -1.0029e+00, -4.6438e-01,\n",
       "        -2.4197e+00, -1.9281e-01, -1.2148e+00, -2.6366e+00,  2.6139e+00,\n",
       "        -1.4514e+00, -9.9477e-01,  1.9075e-01,  1.3353e+00,  1.7402e-01,\n",
       "         2.9062e-01,  1.4426e+00, -3.8899e+00, -2.6767e+00, -5.3622e-01,\n",
       "        -1.2170e+00, -2.7142e+00,  2.7254e-01,  1.1312e-01,  4.1708e+00,\n",
       "        -4.7743e+00, -1.5435e+00, -2.8361e+00, -8.2936e-01,  3.5986e-01,\n",
       "         3.6707e+00,  1.7144e-01, -1.9550e+00,  2.2003e-01,  2.7348e+00,\n",
       "         2.3014e-01, -1.9095e+00,  1.0214e+00,  1.3606e+00,  3.1802e+00,\n",
       "        -1.9440e+00, -3.4443e+00,  1.8772e+00,  2.8991e+00,  3.4642e+00,\n",
       "        -2.3338e+00,  2.0436e+00,  2.7717e+00,  2.8602e+00,  8.9031e-01,\n",
       "         1.4212e-01,  4.6303e+00, -4.5496e-01,  1.1091e+00, -1.3951e+00,\n",
       "         1.9668e+00, -4.0056e-01,  8.7803e-02, -2.8933e-01,  1.4275e+00,\n",
       "         4.8849e-01, -1.2419e+00,  1.2965e+00, -7.0056e-01, -3.9695e+00,\n",
       "        -1.7931e+00, -1.4296e+00,  1.7191e+00, -1.2041e+00, -1.2107e+00,\n",
       "        -9.5997e-01,  2.0049e+00,  7.8577e-01,  2.6662e+00, -3.8454e+00,\n",
       "        -8.3370e-01, -7.4822e-01,  2.0585e+00, -1.9392e+00,  3.7361e-02,\n",
       "         1.4803e-01,  2.5714e+00, -4.1787e+00, -7.3637e-02,  8.1958e-01,\n",
       "        -1.5357e+00,  8.1801e-01, -7.3370e-02, -1.3568e+00,  5.2597e-01,\n",
       "        -1.5757e+00,  6.3544e-01,  3.2981e+00, -3.0696e-01,  2.3199e+00,\n",
       "        -3.8863e+00, -2.2193e+00,  5.9388e-01, -6.7754e-02, -4.1449e-01,\n",
       "        -5.7867e-01,  1.2596e+00, -3.9716e+00,  2.1166e+00, -3.3390e+00,\n",
       "         2.7899e+00, -2.6018e+00, -2.0263e+00, -2.7949e+00,  1.5117e+00,\n",
       "        -1.8353e+00, -5.5611e-01, -2.4916e+00, -3.1008e+00, -4.4417e+00,\n",
       "         3.8719e+00, -6.5756e-01, -1.2887e+00,  2.5267e+00, -3.1161e+00,\n",
       "         1.6476e+00, -7.8240e-01,  8.7107e-01,  2.3616e-01,  1.1048e+00,\n",
       "        -1.2093e+00, -8.6282e-01, -2.4169e+00, -2.3066e+00,  2.2587e-01,\n",
       "         1.2153e+00, -2.8717e+00,  1.2428e+00, -8.2747e-02, -2.3926e+00,\n",
       "        -3.3298e+00,  7.9704e-01,  1.6848e+00, -4.5690e+00,  1.9171e-01,\n",
       "        -3.7149e+00,  1.9016e+00,  1.7491e+00, -4.0047e-01, -3.2387e+00,\n",
       "         1.7856e+00, -1.6746e+00,  2.9593e+00,  8.3965e-02,  4.8876e+00,\n",
       "        -2.0057e+00,  4.1793e-01, -2.9558e+00, -5.5267e+00,  2.6772e+00,\n",
       "         2.0498e+00,  4.2947e+00, -1.9532e+00, -5.9771e-03, -4.2687e-01,\n",
       "        -6.6595e-01,  1.0242e+00,  1.4319e+00,  4.1622e-01, -2.2630e-01,\n",
       "         1.6571e-01, -9.3128e-01,  1.4855e+00,  1.2892e+00, -7.9555e-01,\n",
       "        -2.1127e+00,  8.5361e-01,  9.6383e-01,  3.7265e+00, -2.9206e+00,\n",
       "         2.2444e-01,  3.8800e+00, -3.1632e+00, -2.1348e+00,  2.8109e+00,\n",
       "         2.2833e+00,  2.0149e+00, -3.3186e+00,  7.7659e-01,  2.2314e+00,\n",
       "        -4.5155e-02, -2.5414e+00,  1.0394e+00, -1.1665e+00,  2.1090e-01,\n",
       "        -2.7997e+00,  1.2560e+00, -4.6348e-02,  4.7162e-01,  1.2329e+00,\n",
       "         3.6983e+00,  2.4521e+00,  1.3248e+00, -6.4794e-02,  9.3575e-01,\n",
       "        -1.5403e+00, -1.9136e+00,  7.9532e-02, -5.7742e-02,  2.3721e+00,\n",
       "         1.1472e-01,  3.4300e+00,  1.8850e+00, -2.5164e+00, -1.1357e-01,\n",
       "        -2.6693e+00,  2.4102e+00,  1.2262e+00, -2.4456e+00, -7.3856e-01,\n",
       "         3.5793e-01, -6.3052e-01,  2.6441e+00, -7.3671e-03, -1.2197e+00,\n",
       "        -1.4425e+00, -7.0563e-02,  2.9400e+00, -4.1017e+00, -3.0026e+00,\n",
       "        -7.0849e-01,  1.5766e+00, -1.8953e+00, -1.3253e+00, -3.4131e+00,\n",
       "        -1.8646e+00,  7.1997e-01,  3.5869e+00,  2.2011e+00, -1.3536e-01,\n",
       "        -8.3016e-01,  5.3707e-01, -1.2688e+00,  1.2622e+00, -7.6105e-01,\n",
       "         2.7600e+00,  1.7328e+00, -9.1615e-01,  2.1800e-01,  8.3116e-01,\n",
       "        -7.6790e-01,  1.6253e+00,  1.9237e+00,  4.9057e+00,  4.6081e+00,\n",
       "         5.1766e-01, -1.0428e+00,  3.9122e+00,  6.9001e-01,  2.3184e+00,\n",
       "         1.2690e+00,  2.5759e+00, -1.9149e+00, -4.0973e-01,  1.9039e+00,\n",
       "        -3.1076e-03, -8.4475e-01, -1.8762e-01, -2.6549e+00, -3.2065e-01,\n",
       "         3.0195e-01,  2.5826e+00, -3.2216e+00,  4.8590e-01,  2.0930e+00,\n",
       "        -2.5609e+00, -1.3651e+00, -6.3942e-01,  3.1201e+00, -1.8426e+00,\n",
       "         1.3616e+00,  4.2152e+00, -1.1713e+00, -6.8595e-01, -1.5949e+00,\n",
       "        -2.7114e+00, -1.0751e+00,  2.0908e+00, -2.8487e+00, -2.5265e+00,\n",
       "        -3.0511e+00, -1.1223e+00, -1.0774e-02, -9.8392e-01, -4.3481e-01,\n",
       "         1.6143e+00,  4.5350e-01, -2.2698e+00,  1.9568e-01,  2.5197e+00,\n",
       "        -1.8242e+00,  4.5773e+00,  1.6615e+00,  2.4115e-02,  7.8860e-01,\n",
       "         3.0926e-01,  1.1430e+00,  7.0973e-01,  6.7880e-01, -2.8212e+00,\n",
       "        -1.2224e+00, -1.9434e+00,  5.2959e-01,  7.0151e-01,  4.6403e+00,\n",
       "         2.5827e+00,  2.1867e+00,  5.7719e-01,  1.4509e+00, -8.5399e-01,\n",
       "         7.8765e-01, -2.3278e-01,  2.6874e+00, -1.5922e+00, -5.9333e-01,\n",
       "         8.9481e-01,  1.8820e+00,  2.3642e+00, -6.7592e-01,  2.9735e+00,\n",
       "        -1.5477e+00, -1.5017e+00, -3.0220e+00,  2.5015e+00, -1.6003e-01,\n",
       "         2.1272e+00, -2.9329e+00, -8.5897e-01,  5.4334e-01,  1.2763e+00,\n",
       "         1.7234e+00,  2.0865e+00,  5.2746e-01, -2.5506e+00,  8.4041e-02,\n",
       "         1.6051e+00, -3.5226e+00,  2.3167e+00, -9.7591e-02, -4.1291e-01,\n",
       "        -2.2335e-02,  6.6650e-01,  1.9282e+00,  9.0023e-01,  3.6622e+00,\n",
       "        -3.2966e+00,  1.1247e+00,  3.0606e+00,  1.6084e+00,  1.7743e-01,\n",
       "        -6.6745e-01, -2.1719e+00,  2.5007e+00, -1.3913e+00,  6.7092e-01,\n",
       "        -2.9914e-01, -3.3252e+00,  1.5303e+00,  9.3427e-01, -1.3260e+00,\n",
       "        -2.2827e+00, -3.1851e+00,  1.0397e+00, -1.0854e+00, -8.0418e-01,\n",
       "         8.8446e-01, -4.7630e+00,  1.0992e+00,  1.8096e+00,  6.3814e-01,\n",
       "        -2.5800e+00, -3.2907e+00, -1.6844e+00,  1.4667e+00,  7.9874e-01,\n",
       "         1.8660e+00, -2.7319e-01, -5.8623e-01,  3.1043e-03,  1.4189e+00,\n",
       "         1.7384e+00,  2.4421e+00, -1.2662e+00, -4.0896e-01, -1.7438e+00,\n",
       "         2.3190e+00, -4.6759e+00,  8.7368e-01,  2.3224e+00,  5.0043e-01,\n",
       "        -2.8477e+00,  6.3956e-01,  4.6788e+00,  3.5246e-01,  1.6627e+00,\n",
       "        -9.5108e-01, -1.8815e+00, -1.5241e-01, -6.2155e-01,  2.5272e+00,\n",
       "         1.6775e+00,  3.9964e-01, -3.5047e+00,  2.1351e+00,  1.8841e+00,\n",
       "         1.2637e+00, -7.6598e-01, -6.7874e-01,  2.0728e+00, -2.2545e+00,\n",
       "        -1.3734e+00, -5.0584e-01, -2.3235e+00,  3.4692e+00, -2.1021e+00,\n",
       "        -1.3835e+00, -1.3520e+00, -1.2594e+00, -1.6034e+00,  2.3683e+00,\n",
       "         1.8760e+00, -2.6227e+00, -4.1694e-01, -2.4752e+00,  3.0145e+00,\n",
       "         1.8000e+00,  7.6340e-01,  3.3218e-01,  2.8623e+00, -1.8191e+00,\n",
       "         1.0042e+00, -1.2304e+00,  2.0419e-01, -1.9915e+00, -1.3080e+00,\n",
       "         1.9373e+00, -4.0901e+00,  5.0842e+00,  1.7555e+00, -3.1513e+00,\n",
       "         1.7149e+00, -2.9450e-01,  1.2456e+00, -6.5765e-01,  6.7783e-01,\n",
       "        -2.0392e+00, -1.1990e+00, -5.7708e-01, -3.0369e+00,  1.0053e-01,\n",
       "        -4.1548e-01,  4.5285e+00, -2.6783e+00, -6.3675e-01, -1.0686e+00,\n",
       "         8.2583e-01, -1.4525e+00, -3.6796e-01, -1.8157e+00,  1.2085e+00,\n",
       "         2.0382e+00, -2.6375e+00, -7.0881e-01,  4.7067e+00, -1.2813e+00,\n",
       "         3.4234e-01,  2.5337e+00, -1.4937e+00,  4.7866e+00,  3.6238e+00,\n",
       "        -1.1060e+00, -1.3298e+00, -4.7855e-01,  1.3414e+00,  8.9278e-01,\n",
       "         7.8211e-02, -2.3842e+00, -1.8530e+00, -1.4058e+00,  2.1484e+00,\n",
       "        -4.7814e+00,  2.4317e+00, -1.4604e+00,  1.5495e+00,  6.4187e-01,\n",
       "        -1.2284e+00,  3.6501e-01, -1.8302e-01,  1.5887e+00, -8.8537e-01,\n",
       "         1.8584e+00, -1.8421e+00, -4.3747e-01, -9.8029e-01,  8.2443e-02,\n",
       "         1.5781e+00,  1.4222e+00,  1.2772e+00,  3.7899e+00,  1.0813e+00,\n",
       "        -1.6215e+00, -2.1541e+00,  2.1839e+00, -1.3973e+00,  9.5483e-01,\n",
       "        -2.1103e+00, -3.0371e+00,  1.2741e-01,  2.6508e+00,  8.8140e-01,\n",
       "        -3.1948e+00,  3.0439e+00,  2.3602e+00, -7.9971e-01, -2.1810e+00,\n",
       "         3.4630e+00,  2.3343e+00,  1.7389e+00,  1.7426e+00,  4.3072e-01,\n",
       "         1.3041e+00,  7.3649e-01, -1.0190e+00, -2.1242e+00, -1.7028e+00,\n",
       "         8.2523e-01,  3.4517e+00, -9.9888e-01,  1.9725e-01, -2.7161e+00,\n",
       "         1.3192e+00, -1.6688e+00, -3.0773e+00, -3.8764e+00, -1.5107e+00,\n",
       "         7.6754e-01, -2.9511e+00,  1.5153e+00, -2.6388e-02,  9.4365e-01,\n",
       "         1.7018e+00,  1.9486e+00, -1.9887e+00, -7.5229e-01,  7.0199e-01,\n",
       "         1.1118e+00,  6.3754e-01, -3.1702e+00, -6.4307e-01,  1.5020e-01,\n",
       "        -1.7361e+00,  2.5716e+00,  2.9454e+00, -4.9219e-01, -2.2562e-01,\n",
       "        -1.9527e+00, -1.2299e+00, -4.2941e+00,  1.9357e+00,  8.4854e-01,\n",
       "        -1.4464e+00,  2.3674e+00, -1.1732e+00,  3.3524e+00, -1.9318e+00,\n",
       "        -1.2739e+00, -3.9122e+00, -1.9573e-01,  1.9416e-01, -9.2146e-01,\n",
       "        -1.3446e+00, -1.8769e-01, -2.5091e-01, -4.0248e+00, -3.7824e-01,\n",
       "         2.5647e+00,  2.0550e+00,  2.6213e+00, -3.4423e+00, -6.9107e-01,\n",
       "         1.2121e+00, -2.9588e+00,  2.4589e+00, -7.9165e-01,  1.4964e-01,\n",
       "         2.7501e+00, -1.4896e+00,  9.9341e-02, -1.2257e+00, -1.6289e+00,\n",
       "         4.3582e+00, -2.5927e-01, -9.9533e-01,  3.3550e+00,  7.5188e-01,\n",
       "        -1.4956e+00,  8.9223e-01, -1.0259e+00, -2.0891e+00, -9.8495e-01,\n",
       "        -1.2650e+00,  6.1677e-01,  2.0189e+00, -1.1803e+00,  3.6195e-01,\n",
       "        -1.0521e-01, -3.9245e+00, -4.0294e+00,  6.6223e-01, -6.2137e-01,\n",
       "         1.7709e+00, -8.5305e-01,  4.8329e-01,  2.1041e+00,  1.5133e+00,\n",
       "         1.4164e+00, -1.0713e+00, -1.3987e+00, -6.4624e-01, -1.2368e-01,\n",
       "        -2.9718e+00, -1.8609e+00,  2.5752e+00, -2.1207e+00, -2.7086e+00]),\n",
       "indices=tensor([4, 6, 3, 1, 8, 7, 0, 0, 3, 7, 4, 5, 0, 8, 8, 6, 8, 8, 2, 6, 7, 6, 2, 4,\n",
       "        5, 6, 6, 9, 2, 7, 1, 4, 8, 7, 7, 5, 8, 5, 7, 6, 6, 3, 6, 2, 1, 2, 3, 7,\n",
       "        5, 7, 3, 5, 6, 1, 3, 7, 5, 0, 3, 1, 4, 7, 6, 3, 4, 3, 8, 1, 0, 9, 6, 6,\n",
       "        5, 6, 2, 2, 0, 0, 9, 0, 8, 6, 8, 6, 3, 6, 0, 1, 7, 2, 2, 6, 6, 4, 6, 4,\n",
       "        0, 8, 8, 5, 3, 6, 6, 1, 3, 5, 7, 1, 1, 0, 8, 6, 3, 6, 7, 3, 0, 7, 9, 4,\n",
       "        3, 0, 0, 6, 5, 1, 8, 1, 1, 3, 0, 4, 3, 2, 2, 6, 6, 3, 2, 4, 6, 0, 3, 7,\n",
       "        4, 4, 1, 1, 8, 3, 0, 0, 7, 9, 5, 9, 2, 9, 9, 2, 3, 9, 6, 4, 6, 9, 2, 0,\n",
       "        3, 1, 5, 1, 0, 2, 0, 0, 2, 1, 4, 2, 6, 8, 1, 3, 0, 1, 0, 8, 9, 7, 9, 6,\n",
       "        8, 5, 0, 4, 2, 2, 1, 0, 2, 6, 8, 9, 2, 2, 8, 4, 1, 0, 9, 1, 4, 0, 0, 0,\n",
       "        9, 8, 9, 5, 1, 3, 1, 0, 6, 6, 5, 7, 5, 0, 2, 3, 0, 4, 1, 1, 1, 4, 3, 1,\n",
       "        5, 2, 0, 4, 8, 0, 5, 0, 9, 7, 3, 6, 4, 1, 2, 6, 2, 6, 5, 8, 1, 3, 0, 6,\n",
       "        9, 4, 1, 7, 9, 2, 4, 0, 6, 2, 6, 8, 8, 5, 0, 9, 0, 2, 1, 5, 2, 8, 1, 6,\n",
       "        4, 8, 7, 4, 7, 6, 4, 7, 8, 3, 6, 9, 2, 5, 5, 6, 6, 0, 5, 3, 0, 0, 3, 6,\n",
       "        3, 5, 8, 2, 9, 7, 0, 6, 8, 3, 1, 0, 0, 5, 8, 7, 1, 1, 9, 1, 6, 1, 8, 3,\n",
       "        2, 7, 6, 1, 1, 6, 3, 7, 3, 0, 7, 5, 5, 0, 7, 4, 9, 8, 2, 2, 6, 6, 8, 8,\n",
       "        3, 8, 3, 4, 4, 5, 7, 8, 9, 3, 9, 1, 9, 0, 8, 3, 7, 6, 1, 5, 2, 6, 4, 3,\n",
       "        8, 7, 5, 4, 2, 7, 5, 3, 0, 1, 3, 6, 1, 1, 9, 7, 9, 7, 2, 9, 0, 5, 1, 1,\n",
       "        1, 0, 5, 6, 8, 8, 9, 5, 9, 7, 1, 7, 9, 9, 3, 4, 4, 5, 9, 6, 0, 2, 6, 4,\n",
       "        1, 7, 8, 1, 3, 6, 7, 7, 5, 1, 0, 0, 5, 3, 9, 7, 4, 2, 2, 5, 8, 3, 4, 1,\n",
       "        5, 0, 6, 9, 7, 2, 9, 6, 5, 0, 4, 8, 1, 4, 4, 6, 5, 3, 2, 0, 1, 1, 8, 7,\n",
       "        2, 4, 9, 5, 6, 6, 8, 2, 6, 8, 7, 1, 7, 4, 4, 9, 0, 9, 7, 1, 5, 7, 3, 4,\n",
       "        4, 0, 3, 2, 1, 8, 9, 8, 7, 5, 1, 1, 6, 2, 6, 5, 5, 9, 9, 0, 9, 8, 9, 5,\n",
       "        3, 9, 0, 9, 2, 1, 7, 8, 0, 4, 3, 7, 1, 6, 6, 5, 6, 7, 2, 6, 4, 5, 6, 3,\n",
       "        6, 6, 9, 0, 7, 6, 3, 7, 4, 9, 5, 3, 8, 1, 5, 5, 4, 9, 6, 0, 7, 3, 0, 6,\n",
       "        6, 8, 3, 1, 0, 5, 5, 2, 5, 3, 4, 7, 6, 1, 2, 9, 9, 3, 0, 7, 6, 3, 9, 2,\n",
       "        7, 0, 7, 4, 3, 4, 1, 3, 2, 5, 1, 7, 4, 4, 2, 6, 2, 6, 5, 3, 6, 6, 2, 0,\n",
       "        3, 4, 7, 7, 5, 2, 6, 1, 7, 1, 6, 6, 2, 6, 0, 0, 4, 9, 2, 8, 2, 3, 1, 6,\n",
       "        7, 5, 3, 8, 7, 4, 9, 4, 7, 4, 2, 6, 6, 4, 7, 3, 8, 5, 5, 4, 3, 5, 2, 3,\n",
       "        4, 7, 6, 4, 0, 2, 3, 7, 2, 7, 8, 2, 7, 4, 7, 4, 2, 1, 8, 2, 3, 8, 2, 5,\n",
       "        4, 7, 6, 6, 7, 4, 0, 5, 7, 0, 2, 9, 3, 6, 0, 9, 2, 1, 7, 0, 7, 0, 5, 1,\n",
       "        1, 3, 5, 5, 5, 3, 4, 7, 9, 2, 8, 0, 5, 9, 0, 6, 8, 0, 6, 2, 9, 3, 0, 1,\n",
       "        8, 7, 5, 4, 2, 8, 0, 8, 0, 8, 8, 8, 1, 5, 4, 7, 2, 8, 2, 1, 7, 2, 8, 5,\n",
       "        4, 2, 3, 4, 1, 6, 6, 0, 1, 3, 0, 2, 1, 0, 3, 0, 2, 3, 7, 0, 2, 3, 9, 0,\n",
       "        0, 6, 8, 2, 3, 4, 4, 7, 1, 0, 1, 7, 2, 8, 0, 4, 8, 7, 4, 4, 1, 3, 5, 0,\n",
       "        2, 3, 6, 8, 7, 4, 4, 9, 3, 6, 5, 1, 9, 6, 9, 9, 8, 7, 7, 5, 8, 3, 8, 4,\n",
       "        9, 1, 6, 2, 2, 3, 0, 7, 5, 9, 2, 6, 2, 7, 7, 9, 4, 3, 8, 4, 0, 5, 8, 2,\n",
       "        0, 6, 1, 6, 8, 2, 9, 5, 8, 9, 9, 3, 0, 8, 4, 7, 5, 0, 0, 4, 3, 0, 9, 8,\n",
       "        9, 3, 4, 6, 2, 5, 9, 4, 3, 5, 0, 5, 7, 7, 3, 6, 3, 5, 4, 7, 8, 6, 4, 2,\n",
       "        8, 8, 2, 0, 4, 9, 7, 6, 9, 7, 9, 8, 9, 2, 2, 9, 9, 1, 7, 1, 9, 3, 4, 8,\n",
       "        7, 8, 5, 9, 4, 1, 3, 1, 7, 9, 2, 3, 0, 2, 7, 0, 0, 2, 4, 1, 1, 3, 3, 3,\n",
       "        1, 8, 3, 6, 3, 4, 6, 3, 0, 3, 1, 1, 6, 4, 4, 0, 9, 5, 8, 5, 4, 6, 3, 7,\n",
       "        4, 2, 2, 1, 6, 8, 7, 7, 2, 3, 7, 3, 7, 7, 7, 2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations.median(-1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
